{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d668113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunctional中定义了与大多数layer对应的函数，它与Module的区别：nn.Module实现的层（layer）是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数\\nnn.functional中的函数更像是纯函数,只是进行简单的数学运算而已。即 functional 中的函数是一个确定的不变的运算公式，输入数据产生输出就ok\\n从上可知，如果模型有可学习的参数，最好使用nn.Module对应的相关layer，否则二者都可以使用，没有什么区别。比如Relu其实没有可学习的参数，只是进行一个运算而已，所以使用的就是functional中的relu函数，而卷积层和全连接层都有可学习的参数，所以用的是nn.Module中的类。\\n\\n不具备可学习参数的层，将它们用函数代替，这样可以不用放在构造函数中进行初始化。\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "'''\n",
    "functional中定义了与大多数layer对应的函数，它与Module的区别：nn.Module实现的层（layer）是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数\n",
    "nn.functional中的函数更像是纯函数,只是进行简单的数学运算而已。即 functional 中的函数是一个确定的不变的运算公式，输入数据产生输出就ok\n",
    "从上可知，如果模型有可学习的参数，最好使用nn.Module对应的相关layer，否则二者都可以使用，没有什么区别。比如Relu其实没有可学习的参数，只是进行一个运算而已，所以使用的就是functional中的relu函数，而卷积层和全连接层都有可学习的参数，所以用的是nn.Module中的类。\n",
    "\n",
    "不具备可学习参数的层，将它们用函数代替，这样可以不用放在构造函数中进行初始化。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d47e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0874, -0.0468,  0.0266, -0.0663, -0.2114,  0.2140, -0.1899,  0.3085,\n",
       "         -0.2459,  0.2220],\n",
       "        [ 0.2364, -0.1188, -0.0205,  0.0296, -0.0666,  0.1892, -0.1022,  0.0814,\n",
       "         -0.2107,  0.2139]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "X=torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e52bb",
   "metadata": {},
   "source": [
    "```nn.Sequential```定义了一种特殊的Module，任何一个层和神经网络都是Module的一个子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a1f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c5459",
   "metadata": {},
   "source": [
    "自定义的MLP是nn.Module的子类，所有Module的两个重要函数，\\_\\_init\\_\\_和forward,\\_\\_init\\_\\_中定义了神经网络的所有层，forward从给定输入得到输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4d0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
